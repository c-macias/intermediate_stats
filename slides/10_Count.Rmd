---
title: "Categorical data and regression"
author: "Frank Edwards"
output: binb::metropolis
---

```{r setup, echo = FALSE, message = FALSE}
rm(list=ls())
library(tidyverse)
library(broom)
select<-dplyr::select
set.seed(1)

options(xtable.comment = FALSE)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE, size = "small")
```

## Count data

- Counts are cumulative totals of the number of incidences of some event, generally across time or place
- Counts are positive integers $\in [0,\infty]$
- We can generally express counts as rates by dividing by the desired exposure variable (e.g. population, time, subject)

## Counts as extensions of binary data

- Counts can be thought of as repeated binary trials
- $\sum{y_i}$ where y is equal to 1 or 0 provides a count
- Generally, we could treat \texttt{sum(y==1) + sum(y==0)} or \texttt{nrow(y)} as the exposure, or denominator for a rate. Why?

## Let's return to the Titanic
```{r message = FALSE, size = "tiny"}
titanic<-read_csv("./data/titanic.csv")%>%
  select(-Name)%>%
  rename_all(tolower)%>%
  rename(sibs = `siblings/spouses aboard`,
         kids = `parents/children aboard`)

head(titanic)
```

## Exploring the count variables
```{r}
ggplot(titanic, aes(x = sibs)) + 
  geom_histogram()
```

## Exploring the count variables
```{r}
ggplot(titanic, aes(x = kids)) + 
  geom_histogram()
```

## How can we model kids?

- Perhaps the number of kids is a function of age and sex? Seems reasonable

$$kids_i = \beta_0 + \beta_1 age_i + \beta_2 sex_i + \varepsilon_i$$ 
$$\varepsilon \sim Normal(0,\sigma^2) $$

## Estimating the model
```{r}
m0<-lm(kids ~ sex + age, data = titanic)
m0_out<-tidy(m0)
m0_out
```

## Does the model fit the data? Make prediction data
```{r}
age<-rep(0:79,2)
sex<-rep(c("male", "female"), each=80)
newdata<-data.frame(age = age, sex = sex)
yhat<-predict(m0, newdata = newdata, interval = "confidence")
yhat<-as.data.frame(yhat)
yhat$sex<-sex
yhat$age<-age
```

## Check the fit of the fake data
```{r size = "tiny"}
ggplot(yhat, aes(x = age,
                 y = fit, 
                 ymin = lwr, 
                 ymax = upr,
                 fill = sex))+
  geom_ribbon()
```

## What about with the observed data?
```{r size = "scriptsize"}
newdata<-predict(m0, newdata=titanic, interval = "confidence")
newdata<-as.data.frame(newdata)
newdata$age<-titanic$age
newdata$sex<-titanic$sex
newdata$kids<-titanic$kids
```

## Any problems here?
```{r size = "tiny"}
ggplot(newdata, aes(x = age,
                 y = fit, 
                 ymin = lwr, 
                 ymax = upr,
                 fill = sex))+
  geom_ribbon()+
  geom_point(aes(x = age, y = kids),
             alpha = 0.6, size = 0.3)
```

## Let's try a different approach

```{r size = "scriptsize"}
m1<-glm(kids ~ sex + age, data = titanic, family = "poisson")
newdata<-predict(m1, newdata=titanic, se.fit = TRUE, type = "response")
newdata<-as.data.frame(newdata)
newdata$age<-titanic$age
newdata$sex<-titanic$sex
newdata$kids<-titanic$kids
```

## Somewhat better? How?
```{r size = "tiny"}
ggplot(newdata, aes(x = age,
                 y = fit, 
                 ymin = fit + 2*se.fit, 
                 ymax = fit - 2*se.fit,
                 fill = sex))+
  geom_ribbon()+
  geom_point(aes(x = age, y = kids),
             alpha = 0.6, size = 0.3)
```

# Approaches to modeling count data

## The Poisson model

Where y is a non-negative integer (count)

$$y \sim Poisson (\lambda)$$
$$E(y) = \bar{y} = \lambda$$
$$Var(y)=\lambda $$
$$Pr(y = k) = \frac{\lambda^k e^{-\lambda}}{k!} $$

## Shape of the Poisson distribution
```{r, echo = FALSE}
p1<-rpois(10000, 1)
p2<-rpois(10000, 2)
p3<-rpois(10000, 3)
p4<-rpois(10000, 4)
p5<-rpois(10000, 5)
p6<-rpois(10000, 6)
p7<-rpois(10000, 7)
p8<-rpois(10000, 8)
p9<-rpois(10000, 9)


pois_demo<-data.frame(count = c(p1, p2, p3, p4, p5, p6, p7, p8, p9), 
                      lambda = rep(1:9, each = 10000))
```

```{r}
ggplot(pois_demo, aes(x=count)) + 
  geom_density(adjust = 1/4) + 
  facet_wrap(~lambda)
```

## Let's look at each Poisson variable

```{r size = "scriptsize"}
pois_demo%>%group_by(lambda)%>%
  
  summarise(mean = mean(count),
            variance = var(count))
```

## Poisson models as a GLM

For a count variable $y$, we can specify a Poisson GLM with a log link function

$$ y \sim Poisson(\lambda) $$
$$ \lambda = \beta X = \beta_0 + \beta_1 x_1 \cdots \beta_n x_n $$
$$E(y|x) = e^\lambda $$
$$log(E(y|x)) = \lambda = \beta X $$

## Returning to the titanic (again)

```{r size = "scriptsize"}
tidy(m1)
```

$$E(y|age = age_i, sex = sex_i) = exp(0.27 - 0.93 sex_i - 0.03 age_i$$  

## Turning this into a prediction

$$E(y|age = age_i, sex = sex_i) = exp(0.27 - 0.93 (sex_i=male) - 0.03 age_i)$$  

If age = 20 and sex = female \pause

$$E(y|x) = exp(0.27 - 0.6) = 0.72 = \lambda$$

## What does this look like as a count?

```{r fig.height = 3}
rpois(1, lambda = exp(0.27 - 0.6))
rpois(10, lambda = exp(0.27 - 0.6))
qplot(rpois(1000, lambda = exp(0.27 - 0.6)))
```

## Advantages of the Poisson distribution for regression

1. Constrained to non-negative integers
2. Variance scales with the expectation of y 
3. Relatively simple to interpret

However: 

$$\lambda = E(y|x) = var(y)$$

Is a pretty strong assumption that is rarely true. Let's check it on the Titanic

## Overdispersion in count models

What if we did this:

$$\lambda = E(y|x) $$
$$ var(y)  = \phi \lambda $$
We can scale the variance by a parameter $\phi$ to create overdispersion, or more variance than we might expect under a standard model. 

\emph{In real world settings, there is virtually always overdispersion}

## Quick check for overdispersion

First let's check the mean:

```{r size = "tiny"}
mean(titanic$kids) #observed
mean(exp(predict(m1))) #predicted
```
\pause Now let's check the variance:

```{r size = "tiny"}
var(titanic$kids) #observed variance
var(exp(predict(m1))) #predicted variance
```

Notice any problems?

## Let's compare the histograms

```{r echo = FALSE}
n<-nrow(titanic)
preds<-rpois(n, mean(exp(predict(m1))))
plot_data<-data.frame(kids = c(preds, titanic$kids), type = rep(c("predicted", "observed"), each = n))

ggplot(plot_data, aes(x = kids)) + 
  geom_histogram() + 
  facet_wrap(~type)
```

## Modeling the overdispersion

We have two primary options for modeling overdispersion 

- The quasipoisson model (including $\phi$ to scale variance as an overdispersion parameter)
- The negative binomial model (more on this in a bit)

## Quasipoisson

```{r size = "tiny"}
m2<-glm(kids ~ sex + age, family = "quasipoisson", data = titanic)
summary(m2)
```

## The regular Poisson model on the same data
```{r size = "tiny"}
summary(m1)
```

## Checking predictions again

```{r echo = FALSE}
rqpois <- function(n, mu, theta) {
  rnbinom(n = n, mu = mu, size = mu/(theta-1))
}
n<-nrow(titanic)
preds<-rqpois(n, mean(exp(predict(m2))), theta = summary(m2)$dispersion)
plot_data<-data.frame(kids = c(preds, titanic$kids), type = rep(c("predicted", "observed"), each = n))

ggplot(plot_data, aes(x = kids)) + 
  geom_histogram() + 
  facet_wrap(~type)
```

## A simple set of predictions to interpret the model

```{r size = "tiny", fig.height = 3}
newdata<-data.frame(sex = rep(c("male", "female")), 
                    age = rep(c(5, 25, 50), each = 2))

preds<-predict(m2, newdata, type = "response", se.fit = TRUE)

newdata$preds<-preds$fit
newdata$se<-preds$se.fit
ggplot(newdata, aes(x = age, y = preds, color = sex,
                    ymax = preds + se * 2,
                    ymin = preds - se *2)) + 
  geom_point() + 
  geom_linerange()
```

## But we can also get the gist from the coefficients

```{r}
tidy(m2)
```

## An alternative: negative binomial regression

The Negative Binomial distribution is analagous to an overdispersed Poisson regression

```{r}
library(MASS)
select<-dplyr::select
m3<-glm.nb(formula = kids ~ sex + age,  data = titanic)
tidy(m3)
```

## One more example

This is the data I use for this paper: https://ajph.aphapublications.org/doi/abs/10.2105/AJPH.2018.304559
```{r size = "tiny"}
fe<-read_csv("./data/fe_division_rural.csv")
head(fe)
```

*Variables*: county fips code, state, male population by race/ethnicity, urban/rural code, census division, men killed by police use of force by race/ethnicity

## What if counts have a theoretical limit that varies by unit of observation?

How many many men could theoretically be killed in county x in year y? \pause

We may want to bound our regression model by the size of the population. 

## Exposure variables in count models

We can adjust our model to offset for exposure, or roughly how many trials there could be across units.

$$log(E(Y|X)) - log(exposure) log \frac{E(Y|X)}{exposure} = \beta X $$ \pause

If exposure = population, then conveniently, we are modeling per capita rates

## Using offset to include exposure in a model

We estimate a model that predicts total police killings as a function of county metropolitan type. 

Why would we want an offset in this model? \pause

```{r}
m1<-glm(d.total ~ ur.code + 
          
          offset(log(tot.men)), 
        
        data = fe, family = "quasipoisson")
```

## Zero-inflated count models

If you suspect your data may have two processes, one that determines if the outcome is zero or greater than zero, and one that determines the value of a count we can use a two-stage model. 

1. First, estimate a logistic regresion where your outcome y = 1 if the count z > 0
2. Second, estimate a count model on the subset of the data where z > 0

Example: Predicting incarceration length in days from a sample of the general population. We may first want to predict the likelihood that incarceration days > 0, as people must be arrested or conviceted to be incarcerated. There are two data generating processes here: 1) arrest/conviction, 2) sentence/pre-trial detention

## Count models: general guidance

- If your data are non-zero integers, count models are generally more appropriate than OLS
- Default to an overdispersed model (quasipoisson, negative binomial)
- Unless you are certain the outcome is not overdispersed, do not trust standard errors or p-values from Poisson models without overdispersion parameters
- Count models use a log link function - $\beta$ can generally be interpreted as a unit change in x predicts a percent change in y.
- Include an offset with an exposure variable when the number of trials differs across units (e.g. population size varies)

# Review HW7, rough draft guidelines