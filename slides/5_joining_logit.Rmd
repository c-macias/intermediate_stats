---
title: "Joining data - Logistic regression"
author: "Frank Edwards"
date: "2/22/2019"
output: binb::metropolis
---

```{r setup, echo = FALSE, message = FALSE}
library(tidyverse)
library(gapminder)
library(broom)
set.seed(1)

options(xtable.comment = FALSE)
theme_set(theme_minimal())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE, size = "small")
```

# Joining data with dplyr

## Kinds of join

- Add new rows to a data frame: rbind()
- Add new columns to a data frame: cbind()
- Join full data frames: left_join()

## When to rbind()

Bind rows to a dataframe when you have new observations to append. The number, type, and names of columns *must* be identical.

I find rbind to be most useful when I need to add a single or small set of observations manually to a data frame.

## How to rbind()

```{r}
tail(gapminder)
```

## 
```{r size = "scriptsize"}
new_data<-data.frame("country" = "Zimbabwe", "continent" = "Africa",
                     "year" = 2012, lifeExp = 50.2, pop = 12345678, 
                     gdpPercap = 500)
new_gap<-rbind(gapminder, new_data)
tail(new_gap)
```

## When to cbind()

Bind columns to a dataframe when you have a new variable to append. The number of rows *must* be identical in both objects. 

I use cbind most often when combining vectors to create data frames.

## How to cbind()

```{r}
captain<-c("Kirk", "Picard", "Janeway", "Sisko")
ship<-c("Enterprise", "Enterprise", "Voyager", "Defiant")
star_trek<-data.frame(cbind(captain, ship))
star_trek
```

## Joins

Joins are more flexible than cbind() and rbind(). Joins take two data frames and merge them according to your specification. 

They are powerful tools for flexibly merging datasets with common identifier variables. 

Joins merge data frames based on matching key values.

## Kinds of join: from ?dplyr::join

- inner_join(x,y): Return all rows from x with matching values in y, and all columns from x and y.
- left_join(x,y): My most frequently used join function. Returns all rows from x and all columns from x and y. 
- right_join(x,y): Returns all rows from y and all columns from x and y
- full_join(x,y): Return all rows and columns from both x and y
- semi_join(x,y): Return all rows from x with matching values in y, returning only columns in x
- anti_join(x,y): Return all rows from x where there are not matching values in y, keeping only columns from x

## What do joins do?

```{r}
band_members
band_instruments
```

## 

```{r}
band_members %>% inner_join(band_instruments)
```

##

```{r}
band_members %>% left_join(band_instruments)
```

##

```{r}
band_members %>% right_join(band_instruments)
```

##

```{r}
band_members %>% full_join(band_instruments)
```

## Filtering joins

```{r}
band_members %>% semi_join(band_instruments)
```

## Filtering joins

```{r}
band_members %>% anti_join(band_instruments)
```

## A practical example

```{r size = "tiny"}
g_c<-gapminder%>%
  group_by(continent, year)%>%
  summarise(gdp_mean_continent = mean(gdpPercap))

gapminder %>% left_join(g_c)
```

## Another practical example

```{r echo = FALSE}
pop<-read_csv("./data/pop_nat.csv")
mort<-read_csv("./data/total_mort.csv")
```

```{r size = "tiny"}
head(pop)
head(mort)
```

## Check values on common variables

```{r}
unique(pop$race)
unique(mort$race)
```

## Recode variable for join

```{r}
pop<-pop%>%
  mutate(race = case_when(
    
    race == "amind" ~ "American Indian/AK Native",
    
    race == "black" ~ "African American",
    
    race == "asian" ~ "Asian/Pacific Islander",
    
    race == "latino" ~ "Latinx",
    
    race == "white" ~ "White"
  ))
```

## Check values on common variables

```{r}
unique(pop$sex)
unique(mort$sex)
```

## Check values on common variables

```{r}
unique(pop$year)
unique(mort$year)
```

## Check values on common variables

```{r}
unique(pop$age)
unique(mort$age)
```

## Join population onto mortality

```{r}
mort_join<-mort%>%
  left_join(pop)
```

## Check results
```{r size = "tiny"}
head(mort_join)
nrow(mort)
nrow(mort_join)
nrow(pop)
```

## What about a full_join()?

```{r}
mort_join<-mort%>%
  
  full_join(pop)%>%
  
  arrange(year, age, race, sex)
```

## Check results
```{r size = "tiny"}
head(mort_join)
nrow(mort)
nrow(mort_join)
nrow(pop)
```

## Common join key variables

- Year
- Subject ID
- Geo ID (country, FIPS, state, etc)
- Agency ID
- Anything you want, but probably always a categorical variable

# Break

# Logistic regression

## Read in the data for today

```{r size = "scriptsize"}
admissions <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
head(admissions)
nrow(admissions)
```

## Evaluate distribution of binary admission variable

```{r}
ggplot(admissions, aes(x = admit)) + geom_histogram()
```

## Properties of Bernoulli variables

If $y$ is an i.i.d. Bernoulli variable with probability $p$:

$$y \sim Bernoulli(p)$$
$$\Pr(y=1)=p=1-\Pr(y=0)$$
$$E(y) = \bar{y} = p$$ 
$$Var(y) = p(1-p)$$ 

## Summary of admit: What can we say about the probability of admission?

```{r}
mean(admissions$admit)
sum(admissions$admit==1)/nrow(admissions)
var(admissions$admit)
mean(admissions$admit) * (1 - mean(admissions$admit))
```

## How does GRE relate to admission?

```{r}
ggplot(admissions,
       aes(x = admit, y = gre)) + geom_point() + 
  geom_smooth(method = "lm")
```

## GPA?

```{r}
ggplot(admissions,
       aes(x = admit, y = gpa)) + geom_point() + 
  geom_smooth(method = "lm")
```

## Can we fit a model to predict admission?

```{r size = "scriptsize"}
m1<-lm(admit ~ gre + gpa, 
       data = admissions)
```


```{r echo = FALSE}
new_dat<-data.frame("gre" = mean(admissions$gre), 
                    "gpa" = seq(2,4, 0.01),
                    "rank" = 1)
yhat<-data.frame(predict(m1, interval = "confidence", newdata = new_dat))%>%
  cbind(new_dat)
ggplot(yhat, aes(x = gpa, y= fit, ymin = lwr, ymax = upr)) + 
  geom_line() + 
  geom_ribbon(alpha = 0.5)
```

## Let's try a different approach

```{r size = "scriptsize"}
m2<-glm(admit ~ gre + gpa, 
        data = admissions, 
       family = "binomial")
```

```{r echo = FALSE}
new_dat<-data.frame("gre" = mean(admissions$gre), 
                    "gpa" = seq(2,4, 0.01),
                    "rank" = 1)
yhat<-predict(m2,  
              newdata = new_dat,
              se.fit = TRUE)
yhat_df<-data.frame(fit = exp(yhat$fit)/(1+exp(yhat$fit)), 
                    upr = exp(yhat$fit + 2 * yhat$se.fit) / 
                      (1 + exp(yhat$fit + 2 * yhat$se.fit)), 
                    lwr = exp(yhat$fit - 2 *yhat$se.fit) / 
                      (1 + exp(yhat$fit - 2 *yhat$se.fit)))%>%
  cbind(new_dat)

ggplot(yhat_df, aes(x = gpa, y= fit, ymin = lwr, ymax = upr)) + 
  geom_line() + 
  geom_ribbon(alpha = 0.5)
```

## A generalized linear model

Our linear probability model was:

$$Pr(admit = 1) = \beta_0 + \beta_1GRE + \beta_2GPA + \beta_3Rank + \varepsilon$$

Our logistic regression model takes the form:

$$logit(Pr(admit=1)) =\beta_0 + \beta_1GRE + \beta_2GPA + \beta_3Rank$$

The logit function is our link between the linear predictor term $X \beta$ and the outcome $admit$. 

## The logit function

The logit function transforms a probability value on $[0,1]$ to a continuous distribution

$$logit(p) = log \frac{p}{1-p} $$

## The logit function

```{r}
p<-seq(0,1,0.001)
plot(log(p/(1-p)), pch = ".", p)
```

## Logistic regression is a GLM with a logit link

A generalized linear model with link function $g$ takes the form:

$$g(y) = X \beta$$

For OLS, the link function is the identity function $g(y) = y$

For logistic regression, the link function is the logit function

$$logit(y) = X \beta $$
$$y = logit^{-1}(X \beta) $$

## Defining logit and its inverse

$$logit(p) = log \frac{p}{1-p}$$
$$logit^{-1}(x) = \frac{exp(x)}{exp(x) + 1}$$ 

We can use these functions to transform values back and forth from our logit-linear scale and the probability scale.

## Logistic regression

Uses the logit function to model the probability of a binary outcome being equal to 1. The logit function transforms the bounded interval $[0, 1]$ to a continuous distribution, allowing us to proceed with building a regression model as we ordinarily would.

Logistic regression may have more accurate uncertainty estimates than a linear probability model for binary outcomes. Logistic regression also constrains model predictions to $[0, 1]$.

## Running logistic models in R: the glm() function

```{r}
m1<-glm(admit ~ gpa, data = admissions, family = "binomial")
tidy(m1)
```

How do we interpret the coefficients?

## Common interpretations

- Log odds: $\beta_1$
- Odds ratio: $e^{\beta_1 }$ 
- Probability: $logit^{-1}(x) = \frac{exp(X \beta)}{exp(X \beta) + 1}$

I tend to prefer transforming to a probability scale, as log odds and odds ratios are a bit confusing to define and are not especially intuitive.

## To get predicted probabilities from m1

We need $X \beta$, then apply the logit inverse function

```{r, fig.height=3, size = "scriptsize"}
x<-cbind(rep(1, nrow(admissions)), admissions$gpa)
log_odds<-coef(m1)%*%t(x)
pr_y<-exp(log_odds)/(exp(log_odds) +1)
par(mfrow=c(1,2))
plot(x[,2], log_odds, xlab = "GPA")
plot(x[,2], pr_y, xlab = "GPA")
```

## Alternatively

```{r, fig.height=3, size = "scriptsize"}
log_odds<-predict(m1)
pr_y<-predict(m1, type = "response")
```

```{r, echo = FALSE, fig.height=3, size = "scriptsize"}
par(mfrow=c(1,2))
plot(x[,2], log_odds, xlab = "GPA")
plot(x[,2], pr_y, xlab = "GPA")
```

## Next week

- More on using and interpreting the logit model

- Complete paper proposal due next week: Provide me with a 200 - 500 word abstract that describes your proposed research. This proposal should be accompanied by a description or visualization of key variables if available. If not available, explain your plans for obtaining and working with the data in a timely fashion.

- Hw 4 is also due next week. It's relatively short.