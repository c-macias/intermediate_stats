---
title: "Multiple regression 2"
author: "Frank Edwards"
date: "2/21/2020"
output: binb::metropolis
---

```{r setup, include=FALSE}
library(rethinking)
library(dagitty)
library(gridExtra)
library(tidyverse)
set.seed(1)

knitr::opts_chunk$set(tidy = FALSE)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = F, size = "small")

data("WaffleDivorce")
WaffleDivorce<-WaffleDivorce %>% 
  mutate(A = scale(MedianAgeMarriage),
         D = scale(Divorce),
         M = scale(Marriage))
```

## What does it mean to condition on a variable?

Let's return to the divorce model with this DAG

```{r echo = F, fig.height = 1, fig.width = 2, fig.align='center'}
dagConfound<-dagitty("dag{
  M -> D
  A -> D
  A -> M
}")
coordinates(dagConfound)<-list(x = c(M =0, D =2, A = 1), 
                               y = c(M = 0, D = 0, A = 1))
drawdag(dagConfound)
```

We want to know how much we learn about divorce rates by knowing another variable if:

- We already know marriage rates
- We already know the median age at first marriage

## The single and multiple regressions

```{r size = "tiny"}
mAge<-quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu<-a + bA * A,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)),
  data = WaffleDivorce
)
mMarriage<-quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu<-a + bM * M,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)),
  data = WaffleDivorce
)
mBoth<-quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu<-a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)),
  data = WaffleDivorce
)
```

## The relationship between divorce and marriage

```{r echo = F}
betas <- data.frame(t(coeftab(mMarriage, mBoth)@coefs))
betas$model<-c("mMarriage", "mBoth")
ggplot(data.frame(x = c(-2, 2), y = c(-2, 2)), 
       aes(x = x, y = y)) + 
  geom_blank() + 
  geom_abline(data = betas %>% 
                filter(model == "mMarriage"),
              aes(intercept = a, slope = bM,
                  lty = model)) + 
  ylab("Divorce rate") + 
  xlab("Marriage rate")
```

## The relationship between divorce and marriage

```{r echo = F}
ggplot(data.frame(x = c(-2, 2), y = c(-2, 2)), 
       aes(x = x, y = y)) + 
  geom_blank() + 
  geom_abline(data = betas,
              aes(intercept = a, slope = bM,
                  lty = model)) + 
  ylab("Divorce rate") + 
  xlab("Marriage rate")
```

## The relationship between age and marriage

```{r echo = F}
betas <- data.frame(t(coeftab(mAge, mBoth)@coefs))
betas$model<-c("mAge", "mBoth")
ggplot(data.frame(x = c(-2, 2), y = c(-2, 2)), 
       aes(x = x, y = y)) + 
  geom_blank() + 
  geom_abline(data = betas %>% 
                filter(model == "mAge"),
              aes(intercept = a, slope = bA,
                  lty = model)) + 
  ylab("Divorce rate") + 
  xlab("Median age at marriage")
```

## The relationship between age and marriage

```{r echo = F}
ggplot(data.frame(x = c(-2, 2), y = c(-2, 2)), 
       aes(x = x, y = y)) + 
  geom_blank() + 
  geom_abline(data = betas,
              aes(intercept = a, slope = bA,
                  lty = model)) + 
  ylab("Marriage rate") + 
  xlab("Median age at marriage")
```

## Regressions as conditional expectation engines

- mMarriage tells us $\textrm{E}(D | M)$
- mBoth tells us $\textrm{E}(D | A, M)$ \pause

- mAge tells us $\textrm{E}(D|A)$
- mBoth tells us $\textrm{E}(D|A, M)$ \pause

- Once we know the median age at first marriage, the marriage rate provides little additional information about divorce rates.  \pause
- The association between marriage rates and divorce rates is *spurious*, driven by the underlying $D\leftarrow A \rightarrow M$ relationship

## Visualizing models with more than one predictor

1. **Prior prediction plots**: What are plausible datasets we could observe?
2. Posterior prediction plots: Does the model fit the observed data?
3. Counterfactual plots: What is the effect of X on Y?

## Prior prediction on the standardized scale for one value of the predictors

```{r size = "tiny", fig.height = 3}
p<-extract.prior(mBoth)
### make into a data frame
p<-bind_cols(p)
### simulate from 1 SD below the mean for A, M
A<- -1
M<- -1
p<-p %>% 
  mutate(mu = a + bA * A + bM * M, sigma,
         D = rnorm(1000, mu, sigma))

ggplot(p, aes(x = D)) + 
  geom_density()
```

## Prior prediction on the original scale for one value of the predictors

```{r size = "tiny", fig.height = 3}
p<-p %>% 
  mutate(mu = a + bA * A + bM * M, sigma,
         D = rnorm(1000, mu, sigma),
         D = D * sd(WaffleDivorce$Divorce) + mean(WaffleDivorce$Divorce))

ggplot(p, aes(x = D)) + 
  geom_density()
```

## Prior prediction for all observed values

```{r echo = F}
######## FOR THE FULL OBSERVED DATA
### simulate parameters N times
N<-1000
alpha<-rnorm(N, 0, 0.2)
beta_A<-rnorm(N, 0, 0.5)
beta_M<-rnorm(N, 0, 0.5)
sigma<-rexp(N, 1)
### loop prior prediction over observed data
prior_preds<-list()
for(i in 1:nrow(WaffleDivorce)){
  A<-WaffleDivorce$A[i]
  M<-WaffleDivorce$M[i]
  ### apply linear function to observed data
  mu<-alpha + beta_A * A + beta_M * M
  ### draw from likelihood
  D<-rnorm(N, mu, sigma)
  prior_preds[[i]]<-data.frame(mu = mu, D = D, A = A, M = M)
}
### convert list into data.frame
prior_preds<-bind_rows(prior_preds)
### plot
ggplot(prior_preds,
       aes(x = A, y = D * sd(WaffleDivorce$Divorce) + mean(WaffleDivorce$Divorce))) +
  geom_point(alpha = 0.2)
```

## Visualizing models with more than one predictor

1. Prior prediction plots: What are plausible datasets we could observe?
2. **Posterior prediction plots**: Does the model fit the observed data?
3. Counterfactual plots: What is the effect of X on Y?xq

## Posterior prediction plot

```{r size = "tiny", fig.height = 3}
d_post<-sim(mBoth)
mu_post<-apply(d_post, 2, mean)
pi_d_post<-apply(d_post, 2, PI)
plot_dat<-data.frame(mu = mu_post, 
                     d_upr = pi_d_post[2,], 
                     d_lwr = pi_d_post[1,],
                     d_obs = WaffleDivorce$D,
                     state = WaffleDivorce$Loc)

ggplot(plot_dat, aes(x = d_obs,
                     y = mu, 
                     ymin = d_lwr, 
                     ymax = d_upr)) + 
  geom_pointrange() + 
  geom_abline(intercept = 0, slope = 1) + 
  xlab("observed divorce rate") + 
  ylab("predicted divorce rate")
```

## Posterior prediction plot with labels

```{r size = "tiny", fig.height = 3}
ggplot(plot_dat, aes(x = d_obs,
                     y = mu, 
                     ymin = d_lwr, 
                     ymax = d_upr,
                     label = state)) + 
  geom_linerange(alpha = 0.3) + 
  geom_text() + 
  geom_abline(intercept = 0, slope = 1)+ 
  xlab("observed divorce rate") + 
  ylab("predicted divorce rate")
```

## Visualizing models with more than one predictor

1. Prior prediction plots: What are plausible datasets we could observe?
2. Posterior prediction plots: Does the model fit the observed data?
3. **Counterfactual plots**: What is the effect of X on Y?

## Counterfactual plots

Algorithm:

1. Pick a variable to manipulate
2. Choose a range or set of values for that predictor
3. Simulate each of the other variables in the model for each value of the predictor and each posterior sample

## What is the impact of age at first marriage on divorce?

1. Pick a variable to manipulate: Age
2. Choose a range for that predictor: [-2, 2]

```{r}
## scenarios: age varies from -2 to +2 SD of observed
a_cfact<-seq(-2, 2, length.out = 100)
```
\pause
3. Simulate each of the other variables in the model for each value of the predictor \pause

\pause Recall that our DAG thinks that $A\rightarrow D$, $A \rightarrow M$, and $M\rightarrow D$. If we want to understand what will happen with A changes, we need to allow M to move as A moves.

## Simulate other variables in the model

Model the relationship between A and M on D, and the relationship between A and M

```{r size = "tiny"}
mBothCFact<-quap(
  alist(
    ## A -> D <- M
    D ~ dnorm(mu, sigma),
    mu<-a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1),
    ## A -> M
    M ~ dnorm(mu_m, sigma_m),
    mu_m <- a_m + bAm * A,
    a_m ~ dnorm(0, 0.2),
    bAm ~ dnorm(0, 0.5),
    sigma_m ~ dexp(1)),
  data = WaffleDivorce)
```

## Simulate other variables in the model

Our predictions should account for the expected changes in M when we counterfactually "manipulate" A

```{r size = "tiny"}
summary(mBothCFact)
```

## Generate predictions

```{r size = "tiny"}
sim_dat<-data.frame(A = seq(-2, 2, length.out = 100))
## A is fixed, simulate M then D from posterior
D_cfact<-sim(mBothCFact, data = sim_dat, vars = c("M", "D"))
## compute mean and PI for both M and D predictions
mu_M <-apply(D_cfact$M, 2, mean)
mu_D <- apply(D_cfact$M, 2, mean)
pi_M <-apply(D_cfact$M, 2, PI)
pi_D <- apply(D_cfact$D, 2, PI)
### put it all together for plotting
sim_dat<-sim_dat %>% 
  mutate(mu_M = mu_M,
         mu_D = mu_D,
         D_lwr = pi_D[1,],
         D_upr = pi_D[2,],
         M_lwr = pi_M[1,],
         M_upr = pi_M[2,])
```

## Plot expected changes in D for changes in A

```{r size ="tiny", fig.height = 3}
ggplot(sim_dat,
       aes(x = A, 
           y = mu_D,
           ymin = D_lwr,
           ymax = D_upr)) + 
  geom_line() + 
  geom_ribbon(alpha = 0.5) + 
  ylab("Divorce rate, posterior prediction") + 
  xlab("Median age at first marriage, counterfactual")
```

## Plot expected changes in M for changes in A (for illustration)

Recall that A is negatively associated with M, but once we condition on A, M is not clearly associated with D. We allow A to change M, and M *and* A to change D in this approach

```{r size ="tiny", fig.height = 3}
ggplot(sim_dat,
       aes(x = A, 
           y = mu_M,
           ymin = M_lwr,
           ymax = M_upr)) + 
  geom_line() + 
  geom_ribbon(alpha = 0.5) + 
  ylab("Marriage rate, posterior prediction") + 
  xlab("Median age at first marriage, counterfactual")
```

# Categorical variables

## Kinds of categorical variables

- Binary [T,F] \pause
- Qualitative differences \pause
- Ranked qualitative differences 

## In R

Categorical variables typically take on three formats:

- factor (a linked pair of integers and labels, can be finicky)
- character (label only vectors)
- integers (where labels are implicit) \pause

I typically prefer to work with character vectors (as.character()), but there are cases where each approach has advantages

## Returning to the height data

```{r}
data(Howell1) 
d <- Howell1 
str(d)
d<-d %>% 
  mutate(h = as.vector(scale(height)),
         w = as.vector(scale(weight)))
```
\pause
any categoricals?

## Estimating a model with sex as a predictor

```{r}
m_h1<-quap(alist(
  h ~ dnorm(mu, sigma),
  mu <- a + b * male,
  a ~ dnorm(0, 1),
  b ~ dnorm(0, 1),
  sigma ~ dexp(1)),
  data = d)

summary(m_h1)
```

## What does this model suggest?

```{r size = "tiny", fig.height = 3}
new_data<-data.frame(male = c(0,1))
sim_post<-sim(m_h1, data = new_data)
plot_dat<-data.frame(height = c(sim_post[,1], sim_post[,2]),
                     male = rep(c(0, 1), each = 1000))

ggplot(plot_dat, aes(x = height, fill = factor(male))) + 
  geom_density(alpha = 0.8)
```

## The linear model explained

\[E(height_i) = \alpha + \beta \times Male_i\] \pause

For males:

\[E(height_{male}) = \alpha + \beta \times 1\] \pause

For females:

\[E(height_{female}) = \alpha + \beta \times 0\] \pause

The intercept $\alpha$ then becomes the expected height for females, and $\alpha + \beta$ is the expected height for males. 

## An alternative parameterization

```{r size = "tiny"}
### set sex = 1 for F, = 2 for M
d<-d %>% 
  mutate(sex = case_when(male == 0 ~ 1,
                         male == 1 ~ 2))

m_h2 <- quap(alist(
  h ~ dnorm(mu , sigma), 
  mu <- a[sex],
  a[sex] ~ dnorm(0 ,1), 
  sigma ~ dexp(1)), 
  data=d) 

precis(m_h2, depth = 2)
```

## Extending to multiple category variables

```{r}
data(iris)
head(iris)
```

## What is the expected petal length for each species?

```{r size = "tiny", fig.height = 3}
iris<- iris %>% 
  mutate(p.l = as.vector(scale(Petal.Length)))

m_iris<-quap(alist(
  p.l <- dnorm(mu, sigma),
  mu<- a[Species],
  a[Species] ~ dnorm(0, 1),
  sigma ~ dexp(1)), 
  data = iris)

levels(iris$Species)
plot(precis(m_iris, depth = 2))
```

# Masked relationships

## What is the relationship between milk nutrients and brain size in primates?

You thought we were studying criminal justice?

```{r size = "tiny"}
data(milk)
d <- milk
d<-d %>% 
  mutate(K = as.vector(scale(kcal.per.g)),
         N = as.vector(scale(neocortex.perc)),
         M = as.vector(scale(log(mass)))) %>% 
  filter(!(is.na(K) | is.na(N) | is.na(M))) # remove missings
glimpse(d)
```

## Is milk nutrient density related to percent of the brain that is neocortex? to body weight?

Two initial models, one for brain composition, and one for body mass:

\scriptsize
\[K_i \sim N(\mu, \sigma)\]
\[\mu_i = \alpha + \beta_N N_i\]
\[\alpha \sim N(0,1)\]
\[\beta_N \sim N(0,1)\]
\[\sigma \sim Exp(1)\]

\[K_i \sim N(\mu, \sigma)\]
\[\mu_i = \alpha + \beta_M M_i\]
\[\alpha \sim N(0,1)\]
\[\beta_M \sim N(0,1)\]
\[\sigma \sim Exp(1)\]

## Check the priors

```{r fig.height = 3, size = "tiny"}
plot_dat<-data.frame(a=rnorm(100, 0, 1), 
                     bn = rnorm(100, 0 , 1),
                     x = seq(-2, 2, length.out = 100),
                     y = seq(-2, 2, length.out = 100))

ggplot(plot_dat, aes(x,y)) + 
  geom_blank() + 
  geom_abline(aes(intercept = a, slope = bn), alpha = 0.2)
```

## Maybe rein those in a bit?

Try $\alpha \sim N(0, 0.2)$, $\beta_N \sim N(0, 0.5)$

```{r fig.height = 3, size = "tiny"}
plot_dat<-data.frame(a=rnorm(100, 0, 0.2), 
                     bn = rnorm(100, 0 , 0.5),
                     x = seq(-2, 2, length.out = 100),
                     y = seq(-2, 2, length.out = 100))

ggplot(plot_dat, aes(x,y)) + 
  geom_blank() + 
  geom_abline(aes(intercept = a, slope = bn), alpha = 0.2)
```

## Fit the models

```{r}
mN<-quap(alist(
  K ~ dnorm(mu, sigma),
  mu <- a + bn * N,
  a ~ dnorm(0, 0.2),
  bn ~ dnorm(0, 0.5),
  sigma ~ dexp(1)
), data = d)

mM<-quap(alist(
  K ~ dnorm(mu, sigma),
  mu <- a + bm * M,
  a ~ dnorm(0, 0.2),
  bm ~ dnorm(0, 0.5),
  sigma ~ dexp(1)
), data = d)
```

## Format for visualization

```{r size = "tiny"}
### draw from the posterior for both models: M
sim_seq<-seq(-2, 2, length.out=17)
mu_m<-link(mM, data = list(M = sim_seq))
mu_m_mn<-apply(mu_m, 2, mean)
mu_m_pi<-apply(mu_m, 2, PI)
### N
mu_n<-link(mN, data = list(N = sim_seq))
mu_n_mn<-apply(mu_n, 2, mean)
mu_n_pi<-apply(mu_n, 2, PI)
### format for plotting, -2,2 sequence for M, N, mu and PI, along with observed data. Stack for facets
plot_dat<-data.frame(x = sim_seq,
                     y = mu_m_mn,
                     obs_x = d$M,
                     obs_y = d$K,
                     y_upr = mu_m_pi[2,],
                     y_lwr = mu_m_pi[1,],
                     model = "Log mass only") 
plot_dat<-plot_dat%>% 
  bind_rows(data.frame(x = sim_seq,
                       y = mu_n_mn,
                       obs_x = d$M,
                       obs_y = d$K,
                       y_upr = mu_n_pi[2,],
                       y_lwr = mu_n_pi[1,],
                       model = "Percent neocortex only"))
```

## Plot it

```{r size = "tiny", fig.height = 3}
ggplot(plot_dat, aes(x = x, y = y, ymin = y_lwr, ymax = y_upr)) + 
  geom_line() + 
  geom_ribbon(alpha = 0.2) + 
  geom_point(aes(x = obs_x, y = obs_y)) +
  facet_wrap(~model)
```

## Masking

```{r}
d %>% 
  summarise(K_N = cor(K, N),
            N_M = cor(N, M),
            K_M = cor(K, M))
```
\pause

When two predictor variables are correlated with each other, and have opposite sign correlations with the outcome, excluding one can *mask* an underlying relationship. 

## Fit a model with both mass and percent neocortex

```{r size = "tiny", fig.height = 4}
mBoth<-quap(alist(
  K ~ dnorm(mu, sigma),
  mu <- a + bn * N + bm * M,
  a ~ dnorm(0, 0.2),
  bn ~ dnorm(0, 0.5),
  bm ~ dnorm(0, 0.5),
  sigma ~ dexp(1)
), data = d)

plot(coeftab(mM, mN, mBoth))
```

## Multicollinearity

- Sometimes, adding more predictors *unmasks* relationships
- Sometimes, adding more predictors *masks* relationships

```{r fig.height = 3, size = "tiny"}
library(gridExtra)
p1<-ggplot(d, aes(x = kcal.per.g, y = perc.fat)) + geom_point() + geom_smooth(method = "lm")
p2<-ggplot(d, aes(x = kcal.per.g, y = perc.lactose)) + geom_point() + geom_smooth(method = "lm")
p3<-ggplot(d, aes(x = perc.lactose, y = perc.fat)) + geom_point() + geom_smooth(method = "lm")
grid.arrange(p1, p2, p3, ncol = 3)
```

## What happens when we put strongly correlated variables in a model?

```{r size = "tiny"}
d<-d %>% mutate(F = as.vector(scale(perc.fat)), L = as.vector(scale(perc.lactose)))
mF<-quap(alist(
    K ~ dnorm(mu, sigma),
    mu <- a + bF * F,
    a ~ dnorm(0, 0.2),
    bF ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d)

mL<-quap(alist(
    K ~ dnorm(mu, sigma),
    mu <- a + bL * L,
    a ~ dnorm(0, 0.2),
    bL ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d)
```

## Results from one variable regressions

```{r echo = F}
plot(coeftab(mF, mL))
```

## With both variables as predictors


```{r echo = F}
mFL<-quap(alist(
    K ~ dnorm(mu, sigma),
    mu <- a + bF * F+ bF * L,
    a ~ dnorm(0, 0.2),
    bF ~ dnorm(0, 0.5),
    bL ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d)

plot(coeftab(mF, mL, mFL))
```