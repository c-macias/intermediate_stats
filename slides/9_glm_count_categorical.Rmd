---
title: "Generalized Linear Models, part 2"
author: "Frank Edwards"
date: "3/27/2020"
output: binb::metropolis
---

```{r setup, include=FALSE}
library(rethinking)
library(gridExtra)
library(tidyverse)
library(broom)
set.seed(1)

knitr::opts_chunk$set(tidy = FALSE)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = F, size = "small")
```

# Generalized linear models

## The components of a model

- Likelihood of the data (the small-world data generator): *e.g.* $y \sim Binomial(n, p)$
- The linear function and link function (converts likelihood parameters to linear combinations of predictors): *e.g.* $logit(p) = \alpha + \beta x$
- Priors (our initial beliefs about plausible parameter values) *e.g.*:

\[\alpha \sim Normal(0, 1)\]
\[\beta \sim Normal(0, 1)\]

## The Binomial count model

When $n=1$, a Binomial regression model with a logit link function is called a logistic regression. When $n>1$, we are instead modeling counts of events as our outcome.

The expected value for a Binomial variable with probability $p$ and number of trials $n$ is $np$, and the variance is $np(1-p)$. \pause

When $n$ is large, and $p$ is small, $np \approx np(1-p)$. This special case of the Binomial distribution is the Poisson distribution.

## The Poisson likelihood

Where y is a non-negative integer (count)

$$y \sim Poisson (\lambda)$$
$$E(y) = \bar{y} = \lambda$$
$$Var(y)=\lambda $$

## The Poisson likelihood

```{r, echo = FALSE}
p1<-rpois(10000, 1)
p2<-rpois(10000, 2)
p3<-rpois(10000, 3)
p4<-rpois(10000, 4)
p5<-rpois(10000, 5)
p6<-rpois(10000, 6)
p7<-rpois(10000, 7)
p8<-rpois(10000, 8)
p9<-rpois(10000, 9)

pois_demo<-data.frame(count = c(p1, p2, p3, p4, p5, p6, p7, p8, p9), 
                      lambda = rep(1:9, each = 10000))
```

```{r fig.height = 4}
ggplot(pois_demo, aes(x=count)) + 
  geom_density(adjust = 1/4) + 
  facet_wrap(~lambda)
```

# Let's look at each Poisson variable

```{r size = "scriptsize"}
pois_demo%>%group_by(lambda)%>%
  
  summarise(mean = mean(count),
            variance = var(count))
```

## Poisson as a GLM

For a non-negative integer $y$

\[y_i \sim Poisson(\lambda)\]
\[\log\lambda_i = \alpha + \beta X\]

Using appropriate priors for $\alpha$ and $\beta$

## The data for today: Fatal Encounters

Let's use data on police-involved killings to estimate how many men of different racial groups are killed by police in the average US county.

```{r}
fe<-read_csv("./fe_demo.csv")
### on your computer, run fe<-read_csv("./slides/fe_demo.csv")
head(fe)
```

## A mortality model using a Poisson likelihood

Let's define a Poisson regression of death rates $d$

\[d_i \sim Poisson(\lambda)\]
\[\log \lambda_i = \alpha[race]_i \]
\[\alpha[race] \sim Normal(0, 2)\]

## Estimation

```{r results = "hide"}
fe_model<-fe %>% 
  mutate(race_ethn = factor(race.ethn),
         log_pop = log(pop + 1)) %>% 
  select(deaths, race_ethn, log_pop)

fe_pois<-ulam(alist(
  deaths ~ dpois(l),
  log(l) <-  a[race_ethn],
  a[race_ethn] ~ dnorm(0,2)
), data = fe_model, chains = 4, cores = 2)
```

## Check convergence

```{r}
traceplot(fe_pois)
```

## Log-scale parameters are hard to interpret...

```{r size = "tiny"}
precis(fe_pois)
```

## Average expected deaths per county

```{r size = "tiny", fig.height = 3}
post_a<-extract.samples(fe_pois)
post_a<-as.data.frame(post_a)
names(post_a)<-c("black", "latino", "white")
post_a<-post_a %>% 
  pivot_longer(cols = everything(),
               names_to = "race_ethn",
               values_to = "log_lambda") 
# inverse link function to get lambda on E(deaths) scale
post_a<-post_a %>% 
  mutate(lambda = exp(log_lambda)) 

ggplot(post_a,
       aes(x = lambda, fill = race_ethn)) + 
  geom_density(alpha = 0.5)
```

## Problem: these don't adjust for population size 

The standard Poisson model is *unbounded*, and assumes each observation is drawn from a similar unit. However, we can use Poisson models to estimate event *rates* using an *offset*. \pause

An offset (or exposure) scales $\lambda$ to the size of the exposure, turning $\lambda$ into a rate per unit of the offset.

## Including an offset in the model

Let's now estimate a model that includes population $p$ as an exposure variable (typically time, population, or some other measure of event opportunity). 

\[d_i \sim Poisson(\lambda)\]
\[\log\frac{\lambda_i}{p_i} = \log\lambda_i - \log p_i = \alpha + \beta x_i\] \pause

Which we can rewrite as

\[\log \lambda_i = \log p_i + \alpha + \beta x_i\]

We add the offset to the regression without including a parameter.

## Let's re-estimate the model with an offset

\[d_i \sim Poisson(\lambda)\]
\[\log \lambda_i = \log p_i + \alpha[race]_i \]
\[\alpha[race] \sim Normal(0, 2)\]

```{r size = "tiny", results = "hide"}
fe_pois_offset<-ulam(alist(
  deaths ~ dpois(l),
  log(l) <- log_pop + a[race_ethn],
  a[race_ethn] ~ dnorm(0,2)
), data = fe_model, chains = 4, cores = 2)
```

## Interpreting the model, checking convergence diagnostics

```{r size = "tiny"}
precis(fe_pois_offset, depth = 2)
```

## Posterior inference

Let's check the expected number of deaths for each group assuming a population of 100,000. 

```{r size = "tiny"}
sim_dat<-data.frame(race_ethn = 
                       c("black", "latino", "white"),
                    log_pop = log(1e5))

post_lambda<-link(fe_pois_offset, sim_dat)
post_lambda<-as.data.frame(post_lambda)
names(post_lambda)<-c("black", "latino", "white")

post_lambda<-post_lambda %>% 
  pivot_longer(cols = everything(),
               names_to = "race_ethn",
               values_to = "lambda") 
```

## Posterior inference

```{r size = "tiny"}
ggplot(post_lambda,
       aes(x = lambda, fill = race_ethn)) + 
  geom_density(alpha = 0.5)
```

# Categorical regression

## Categorical data

Categorical data falls into a fixed set of categories. It may be \textit{unordered}, meaning that there is no inherent ranking of categories, or it may be \textit{ordered}. Ordered categorical data has an explicit hierarchical ranking of values. 

## Categorical data, examples

Are these variables ordered or unordered? \pause

- Candidate choice in a primary election \pause
- Zip code for people choosing a place to move \pause
- Cause of death \pause
- Opinions on a political issue on a thermometer / Likert scale (e.g. Strongly oppose, oppose, neutral, support, strongly support) \pause
- Graduate program to attend \pause
- Ranking of graduate program 

## Visualizing categorical data, facets

```{r size = "tiny"}
ggplot(iris, aes(x = Petal.Length)) + 
  geom_histogram() + 
  facet_wrap(~Species, ncol=1)
```

## Visualizing categorical data, color

```{r size = "tiny"}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, 
                 color = Species)) + 
  geom_point() 
```

## Predicting Iris species from petal length and width

Can we successfully classify Iris species based on the observed length of the petals?

We will use a *multinomial regression*, which is conceptually similar to running $k-1$ logistic regressions, where $k$ is the number of categories in our outcome.

## Multinomial regression: basics

For a categorical outcome with $K$ categories, estimate $K - 1$ models where 1,2,3 stand in for membership in group 1, 2, 3:

$$log \frac{Pr(y_i =1)}{Pr(y_i=K)} =  \beta x_i$$
$$log \frac{Pr(y_i =2)}{Pr(y_i=K)} =  \beta x_i$$
$$ \cdots$$
$$log \frac{Pr(y_i =K-1)}{Pr(y_i=K)} =  \beta x_i$$

Key assumtion: Independence of irrelevant alternatives. Odds of choice do not depend on the presence or absence of other alternatives (i.e. car vs bus or car vs red bus vs blue bus)

## Implementation 

The book provides details on using Stan directly, but this is tricky with ulam(). I'm using the brms package here (I'll show you more of this later!)

```{r results = "hide"}
library(brms)

iris_model<-brm(Species ~ Petal.Length,
                family = categorical, 
                data = iris)
```

## These are hard to interpret...

```{r size = "tiny"}
summary(iris_model)
```

## Let's predict flower species

```{r fig.height = 4}
conditional_effects(iris_model, categorical = T)
```

## Summary

- We added two new likelihoods into our GLM tool-belt: the Poisson and the multinomial
- Poisson regression can handle counts and rates easily
- Multinomial regression models categorical outcomes
- Homework: Chapter 11 Easy and Medium questions. If you want practice with Poisson models, add 11H4. Note that map == quap and map2stan == ulam.
